{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMHajL_sMlIg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjC6o4hEd9QN"
      },
      "outputs": [],
      "source": [
        "student_id = 2310618 # Note this is an interger and you need to input your id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK54wjsceHd3"
      },
      "outputs": [],
      "source": [
        "# set same seeds for all libraries\n",
        "\n",
        "#numpy seed\n",
        "np.random.seed(student_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHhwJqhweNK8"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx8o-UrMjdy4"
      },
      "source": [
        "Common Codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dgj8NCifGzU"
      },
      "outputs": [],
      "source": [
        "# Add your code to initialize GDrive and data and models paths\n",
        "\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = './CE807-24-SP/Assignment/Lab10/'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))\n",
        "\n",
        "DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'data', '8') # Make sure to replace 0 with last digit of your student Regitration number\n",
        "train_file = os.path.join(DATA_PATH, 'train.csv')\n",
        "print('Train file: ', train_file)\n",
        "\n",
        "val_file = os.path.join(DATA_PATH, 'valid.csv')\n",
        "print('Validation file: ', val_file)\n",
        "\n",
        "test_file = os.path.join(DATA_PATH, 'test.csv')\n",
        "print('Test file: ', test_file)\n",
        "\n",
        "\n",
        "MODEL_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'model', str(student_id)) # Make sure to use your student Regitration number\n",
        "MODEL_Gen_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Gen') # Model Generative directory\n",
        "print('Model Generative directory: ', MODEL_Gen_DIRECTORY)\n",
        "\n",
        "MODEL_Gen_File = MODEL_Gen_DIRECTORY + '.zip'\n",
        "\n",
        "\n",
        "MODEL_Dis_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Dis') # Model Discriminative directory\n",
        "print('Model Discriminative directory: ', MODEL_Dis_DIRECTORY)\n",
        "\n",
        "MODEL_Dis_File = MODEL_Dis_DIRECTORY + '.zip'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOJEeLiGjhMn"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_file)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Count the number of toxic and non-toxic comments\n",
        "num_tox_comm = train_df['toxicity'].sum()\n",
        "num_non_tox_comm = len(train_df) - num_tox_comm\n",
        "\n",
        "# Create a pie chart\n",
        "labels = ['Toxic', 'Non-toxic']\n",
        "sizes = [num_tox_comm, num_non_tox_comm]\n",
        "colors = ['red', 'green']\n",
        "explode = (0.1, 0)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Toxic Comments in the Dataset')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2FSIfs7dz5e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cTL3cgJbJ8R"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "def compute_performance(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates and prints different performance metrics like Accuracy, Recall, Precision, F1 score,\n",
        "    the Confusion Matrix, AUROC (Area Under the Receiver Operating Characteristic Curve), and plots the ROC curve.\n",
        "\n",
        "    Args:\n",
        "        y_true: numpy array or list\n",
        "        y_pred: numpy array or list\n",
        "\n",
        "    Returns:\n",
        "        tuple: (Accuracy, F1 score, Precision, Recall Confusion Matrix)\n",
        "    \"\"\"\n",
        "\n",
        "    # Evaluation metrics\n",
        "    accuracy_file = accuracy_score(y_true, y_pred)\n",
        "    recall_file = recall_score(y_true, y_pred)\n",
        "    precision_file = precision_score(y_true, y_pred)\n",
        "    f1_file = f1_score(y_true, y_pred)\n",
        "    confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Accuracy Score: \", accuracy_file)\n",
        "    print(\"Recall Score: \", recall_file)\n",
        "    print(\"Precision Score: \", precision_file)\n",
        "    print(\"F1 Score: \", f1_file)\n",
        "    print(\"Confusion Matrix: \", confusion_mat)\n",
        "\n",
        "    return accuracy_file, f1_file, precision_file, recall_file, confusion_mat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUcs3nTXburu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def data_commentcleaning(x):\n",
        "    \"\"\"\n",
        "    Perform data cleaning on the input DataFrame x.\n",
        "\n",
        "    Args:\n",
        "        x: DataFrame containing 'comment' column to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        Cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    # Read the input DataFrame (df)\n",
        "    train_data = pd.read_csv(x)\n",
        "\n",
        "    # Convert comment column text to lowercase\n",
        "    train_data['comment'] = train_data['comment'].str.lower()\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    train_data['comment'] = train_data['comment'].apply(lambda x: re.sub(\"[^A-Za-z0-9]\", ' ', x))\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_cleancomment = stopwords.words('english')\n",
        "    train_data['comment'] = train_data['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_cleancomment]))\n",
        "\n",
        "    # Tokenization\n",
        "    tokenizer_cleancomment = RegexpTokenizer(\"[\\w']+\")\n",
        "    train_data['comment'] = train_data['comment'].apply(lambda x: tokenizer_cleancomment.tokenize(x))\n",
        "\n",
        "    # Lemmatize words in the comments column\n",
        "    lemmatizer_cleanedcolumn = nltk.stem.WordNetLemmatizer()\n",
        "    train_data['comment'] = train_data['comment'].apply(lambda x: [lemmatizer_cleanedcolumn.lemmatize(y) for y in x])\n",
        "\n",
        "    # Stemming the words\n",
        "    stemming_cleancomment = SnowballStemmer(\"english\")\n",
        "    train_data['comment'] = train_data['comment'].apply(lambda x: [stemming_cleancomment.stem(y) for y in x])\n",
        "\n",
        "    # Join the list of words back into a single string\n",
        "    train_data['comment'] = train_data['comment'].apply(lambda x: \",\".join(x))\n",
        "\n",
        "\n",
        "    return train_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m39vkUJpkAfN"
      },
      "outputs": [],
      "source": [
        "def save_model(model,model_dir):\n",
        "  # save the model to disk\n",
        "  # Check if the Model directory exists\n",
        "\n",
        "  # Note you might have to modify this based on your requirement\n",
        "\n",
        "  if not os.path.exists(model_dir):\n",
        "      # Create the directory if it doesn't exist\n",
        "      os.makedirs(model_dir)\n",
        "      print(f\"Directory '{model_dir}' created successfully.\")\n",
        "  else:\n",
        "      print(f\"Directory '{model_dir}' already exists.\")\n",
        "\n",
        "  model_file = os.path.join(model_dir, 'model.sav')\n",
        "  pickle.dump(model, open(model_file, 'wb'))\n",
        "\n",
        "  print('Saved model to ', model_file)\n",
        "\n",
        "  return model_file\n",
        "\n",
        "def load_model(model_file):\n",
        "    # load model from disk\n",
        "\n",
        "    # Note you might have to modify this based on your requirement\n",
        "\n",
        "    model = pickle.load(open(model_file, 'rb'))\n",
        "\n",
        "    print('Loaded model from ', model_file)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCLH3CAxkXoD"
      },
      "source": [
        "# Let's download GDrive Link into a directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59kgF7A_kIxL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def extract_file_id_from_url(url):\n",
        "    # Extract the file ID from the URL\n",
        "    file_id = None\n",
        "    if 'drive.google.com' in url:\n",
        "        file_id = url.split('/')[-2]\n",
        "    elif 'https://docs.google.com' in url:\n",
        "        file_id = url.split('/')[-1]\n",
        "\n",
        "    return file_id\n",
        "\n",
        "def download_file_from_drive(file_id, file_path):\n",
        "    # Construct the download URL\n",
        "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    # Download the file\n",
        "    response = requests.get(download_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(\"File downloaded successfully!\",file_path)\n",
        "    else:\n",
        "        print(\"Failed to download the file.\")\n",
        "\n",
        "def download_zip_file_from_link(file_url,file_path):\n",
        "\n",
        "  file_id = extract_file_id_from_url(file_url)\n",
        "  if file_id:\n",
        "      download_file_from_drive(file_id, file_path)\n",
        "  else:\n",
        "      print(\"Invalid Google Drive URL.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D3hVkA9k7xe"
      },
      "source": [
        "# Zip and Unzip a GDrive File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiyp_lx4kQeR"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Function to zip a directory\n",
        "def zip_directory(directory, zip_filename):\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, '..')))\n",
        "        print('Created a zip file',zip_filename)\n",
        "\n",
        "# Function to unzip a zip file\n",
        "def unzip_file(zip_filename, extract_dir):\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print('Extracted a zip file to',extract_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZyGMVrrlUgk"
      },
      "source": [
        "# Get Sharable link of your Zip file in Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VVEpId7k97s"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "def get_gdrive_link(file_path):\n",
        "    # Authenticate and create PyDrive client\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "    # Find the file in Google Drive\n",
        "    file_name = file_path.split('/')[-1]\n",
        "    file_list = drive.ListFile({'q': f\"title='{file_name}'\"}).GetList()\n",
        "\n",
        "    # Get the file ID and generate the shareable link\n",
        "    if file_list:\n",
        "        file_id = file_list[0]['id']\n",
        "        gdrive_link = f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n",
        "        return gdrive_link\n",
        "    else:\n",
        "        return \"File not found in Google Drive\"\n",
        "\n",
        "def get_shareable_link(url):\n",
        "\n",
        "    file_id = extract_file_id_from_url(url)\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "    try:\n",
        "        file_obj = drive.CreateFile({'id': file_id})\n",
        "        file_obj.FetchMetadata()\n",
        "        file_obj.InsertPermission({\n",
        "            'type': 'anyone',\n",
        "            'value': 'anyone',\n",
        "            'role': 'reader'\n",
        "        })\n",
        "\n",
        "        # Get the shareable link\n",
        "        return file_obj['alternateLink']\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FDE7-Jxli7O"
      },
      "source": [
        "#Method Generative Start\n",
        "In this section you will write all details of your Method 1.\n",
        "\n",
        "You will have to enter multiple code and text cell.\n",
        "\n",
        "Your code should follow the standard ML pipeline\n",
        "\n",
        "Data reading\n",
        "Data clearning, if any\n",
        "Convert data to vector/tokenization/vectorization\n",
        "Model Declaration/Initialization/building\n",
        "Training and validation of the model using training and validation dataset\n",
        "Save the trained model\n",
        "Load and Test the model on testing set\n",
        "Save the output of the model\n",
        "You could add any other step(s) based on your method's requirement.\n",
        "\n",
        "After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TejAq_D8li9_"
      },
      "source": [
        "## Training Generative Method Code\n",
        "Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.\n",
        "\n",
        "Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIy4bosQlK1o"
      },
      "outputs": [],
      "source": [
        "def train_Gen(train_file, val_file, model_dir):\n",
        "    \"\"\"\n",
        "    Train the Generative Model using Gaussian Naive Bayes.\n",
        "\n",
        "    Args:\n",
        "        train_file: Path to the training file containing comments.\n",
        "        val_file: Path to the validation file containing comments.\n",
        "        model_dir: Path to the directory to save the trained model.\n",
        "\n",
        "    Returns:\n",
        "        model_gdrive_link: Google Drive link for the trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import libraries\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "    # Perform data cleaning on the train and validation datasets\n",
        "    train_commentdata = data_commentcleaning(train_file)\n",
        "    valid_commentdata = data_commentcleaning(val_file)\n",
        "\n",
        "    # TF-IDF VECTORIZER\n",
        "    global gendis_vectorizer\n",
        "    # Initialize TF-IDF vectorizer\n",
        "    gendis_vectorizer = TfidfVectorizer()\n",
        "    # Perform  Transform train comments into TF-IDF features\n",
        "    X_traincomment = gendis_vectorizer.fit_transform(train_commentdata.comment).toarray()\n",
        "    # Target variable toxicity for training\n",
        "    y_traincomment = train_commentdata.toxicity\n",
        "    # Transform validation comments into TF-IDF features\n",
        "    X_validcomment = gendis_vectorizer.transform(valid_commentdata.comment).toarray()\n",
        "\n",
        "    # Gaussian Naive Bayes Classifier\n",
        "    # Initialize Gaussian Naive Bayes classifier\n",
        "    Gauss_classifier = GaussianNB()\n",
        "    # Train the model\n",
        "    Gauss_classifier.fit(X_traincomment, y_traincomment)\n",
        "    # Predict toxicity for validation comments\n",
        "    y_predcomment = Gauss_classifier.predict(X_validcomment)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    compute_performance(valid_commentdata['toxicity'], y_predcomment)\n",
        "    # Save the Gaussian Naive Bayes model\n",
        "    save_model(Gauss_classifier, model_dir)\n",
        "    # Zip the model directory for sharing\n",
        "    zip_directory(model_dir, MODEL_Gen_File)\n",
        "    # Get the Google Drive link for the zipped model\n",
        "    model_gdrive_link = get_gdrive_link(MODEL_Gen_File)\n",
        "    print(model_gdrive_link)\n",
        "\n",
        "    # Get the shareable link\n",
        "    get_shareable_link(model_gdrive_link)\n",
        "\n",
        "    return model_gdrive_link\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoOJthTFmL1N"
      },
      "source": [
        "## Testing Method 1 Code\n",
        "Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgSvMQ5YmJLc"
      },
      "outputs": [],
      "source": [
        "def test_Gen(test_file, MODEL_PATH, model_gdrive_link):\n",
        "    \"\"\"\n",
        "    Test the Generative Model on the test data.\n",
        "\n",
        "    Args:\n",
        "      test_file: Path to the test file containing comments.\n",
        "      MODEL_PATH: Path to the directory containing the trained model.\n",
        "      model_gdrive_link: Google Drive link for the trained model.\n",
        "\n",
        "    Returns:\n",
        "      test_file: Path to the output file containing the test results.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import necessary libraries\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    import pickle\n",
        "\n",
        "    # Clean the test data\n",
        "    test_gendata = data_commentcleaning(test_file)\n",
        "\n",
        "    # Transform test comments into TF-IDF features\n",
        "    X_gentest = gendis_vectorizer.transform(test_gendata.comment).toarray()\n",
        "\n",
        "    # Paths for temporary directory and file\n",
        "    test_model_file = MODEL_PATH + '/Model_Gen.zip'\n",
        "    test_model_path = MODEL_PATH + '/Model_Gen/'\n",
        "\n",
        "    # Download and unzip the model file\n",
        "    download_zip_file_from_link(model_gdrive_link, test_model_file)\n",
        "    print('Model downloaded to', test_model_file)\n",
        "    unzip_file(test_model_file, test_model_path)\n",
        "    print('\\n Model is downloaded to ', test_model_path)\n",
        "\n",
        "    # Load the trained model\n",
        "    pickle_file = os.path.join(test_model_path, 'model.sav')\n",
        "    model = load_model(pickle_file)\n",
        "\n",
        "    # Predict using the model\n",
        "    y_genpred = model.predict(X_gentest)\n",
        "    test_gendata['out_label_model_Gen'] = y_genpred\n",
        "\n",
        "    # Save the model output in the same output file\n",
        "    test_gendata.to_csv(test_file, index=False)\n",
        "    print('\\n Output is saved in ', test_file)\n",
        "\n",
        "    return test_file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def val_gen(val_file, MODEL_PATH, model_gdrive_link):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "      val_file: has the val file info\n",
        "      MODEL_PATH:Path to the directory containing the trained model.\n",
        "      model_gdrive_link: link for the trained model\n",
        "\n",
        "    Returns: val_file\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Import necessary libraries\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    import pickle\n",
        "\n",
        "    # Clean the test data\n",
        "    val_gendata = data_commentcleaning(val_file)\n",
        "\n",
        "    # Transform test comments into TF-IDF features\n",
        "    X_genval = gendis_vectorizer.transform(val_gendata.comment).toarray()\n",
        "\n",
        "    # Paths for temporary directory and file\n",
        "    test_model_file = MODEL_PATH + '/Model_Gen.zip'\n",
        "    test_model_path = MODEL_PATH + '/Model_Gen/'\n",
        "\n",
        "    # Download and unzip the model file\n",
        "    download_zip_file_from_link(model_gdrive_link, test_model_file)\n",
        "    print('Model downloaded to', test_model_file)\n",
        "    unzip_file(test_model_file, test_model_path)\n",
        "    print('\\n Model is downloaded to ', test_model_path)\n",
        "\n",
        "    # Load the trained model\n",
        "    pickle_file = os.path.join(test_model_path, 'model.sav')\n",
        "    valmodel = load_model(pickle_file)\n",
        "\n",
        "    # Predict using the model\n",
        "    y_genpred = valmodel.predict(X_genval)\n",
        "    val_gendata['out_label_model_Gen'] = y_genpred\n",
        "\n",
        "    # Save the model output in the same output file\n",
        "    val_gendata.to_csv(val_file, index=False)\n",
        "    print('\\n Output is saved in ', val_file)\n",
        "\n",
        "    return val_file\n"
      ],
      "metadata": {
        "id": "VEwotJq_AnQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acz-AREemVKM"
      },
      "source": [
        "## Method Generative End\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQwF19xmYBk"
      },
      "source": [
        "# Method Discriminative Start\n",
        "\n",
        "In this section you will write all details of your Method 2.\n",
        "\n",
        "You will have to enter multiple `code` and `text` cell.\n",
        "\n",
        "Your code should follow the standard ML pipeline\n",
        "\n",
        "\n",
        "*   Data reading\n",
        "*   Data clearning, if any\n",
        "*   Convert data to vector/tokenization/vectorization\n",
        "*   Model Declaration/Initialization/building\n",
        "*   Training and validation of the model using training and validation dataset\n",
        "*   Save the trained model\n",
        "*   Load and Test the model on testing set\n",
        "*   Save the output of the model\n",
        "\n",
        "You could add any other step(s) based on your method's requirement.\n",
        "\n",
        "After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRNnkmogmfYv"
      },
      "source": [
        "## Training Method Discriminative Code\n",
        "Your test code should be a stand alone code that must take `train_file`, `val_file`,  and `model_dir` as input. You could have other things as also input, but these three are must. You would load both files, and train using the `train_file` and validating using the `val_file`. You will `print` / `display`/ `plot` all performance metrics, loss(if available) and save the output model in the `model_dir`.\n",
        "\n",
        "Note that at the testing time, you need to use the same pre-processing and model. So, it would be good that you make those as seperate function/pipeline whichever it the best suited for your method. Don't copy-paste same code twice, make it a fucntion/class whichever is best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYc9v2kTmPaS"
      },
      "outputs": [],
      "source": [
        "def train_dis(train_file, val_file, model_dir):\n",
        "    \"\"\"\n",
        "    Train the discriminative model using Gradient Boosting Classifier.\n",
        "\n",
        "    Args:\n",
        "        train_file: File path of the training data.\n",
        "        val_file: File path of the validation data.\n",
        "        model_dir: Directory path to save the trained model.\n",
        "\n",
        "    Returns:\n",
        "        Google Drive link for the trained model.\n",
        "    \"\"\"\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "    # Perform data cleaning on the training and validation datasets\n",
        "    train_disdata = data_commentcleaning(train_file)\n",
        "    valid_disdata = data_commentcleaning(val_file)\n",
        "\n",
        "    # TF-IDF VECTORIZER\n",
        "    gendis_vectorizer = TfidfVectorizer()\n",
        "    X_distrain = gendis_vectorizer.fit_transform(train_disdata.comment).toarray()\n",
        "    y_distrain = train_disdata.toxicity\n",
        "    X_disvalid = gendis_vectorizer.transform(valid_disdata.comment).toarray()\n",
        "\n",
        "    # Define the pipeline\n",
        "    pipe_dis = Pipeline([('GBC', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=student_id))])\n",
        "    # Train the pipeline\n",
        "    pipe_dis.fit(X_distrain, y_distrain)\n",
        "    # perdict the toxicity\n",
        "    y_dispred = pipe_dis.predict(X_disvalid)\n",
        "\n",
        "    # Save the trained model\n",
        "    save_model(pipe_dis, model_dir)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    compute_performance(valid_disdata['toxicity'], y_dispred)\n",
        "\n",
        "    # Zip the model directory for sharing\n",
        "    zip_directory(model_dir, MODEL_Dis_File)\n",
        "\n",
        "    # Get the Google Drive link for the zipped model\n",
        "    model_gdrive_link = get_gdrive_link(MODEL_Dis_File)\n",
        "    print(model_gdrive_link)\n",
        "\n",
        "    # Get the shareable link\n",
        "    get_shareable_link(model_gdrive_link)\n",
        "\n",
        "    return model_gdrive_link\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQAwsQ1nmwEZ"
      },
      "source": [
        "## Testing Method Discriminative Code\n",
        "Your test code should be a stand alone code that must take `test_file`, `model_file` and `output_dir` as input. You could have other things as also input, but these three are must. You would load both files, and generate output based on inputs. Then you will `print` / `display`/ `plot` all performance metrics, and save the output file in the `output_dir`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsyrcY-AmqEG"
      },
      "outputs": [],
      "source": [
        "def test_dis(test_file, MODEL_PATH, model_gdrive_link):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    Args:\n",
        "        test_file: Path to the test file containing comments.\n",
        "        MODEL_PATH: Path to the directory containing the trained model.\n",
        "        model_gdrive_link: Google Drive link for the trained model.\n",
        "\n",
        "    Returns:\n",
        "        test_file: Path to the output file containing the test results.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import necessary libraries\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    import pickle\n",
        "\n",
        "    # Clean the test data\n",
        "    test_disdata = data_commentcleaning(test_file)\n",
        "\n",
        "    # TF-IDF VECTORIZER\n",
        "    X_disctest = gendis_vectorizer.transform(test_disdata.comment).toarray()\n",
        "\n",
        "    # Paths for temporary directory and file\n",
        "    test_model_file = MODEL_PATH + '/Model_Dis.zip'\n",
        "    test_model_path = MODEL_PATH + '/Model_Dis/'\n",
        "\n",
        "    # Download and unzip the model file\n",
        "    download_zip_file_from_link(model_gdrive_link, test_model_file)\n",
        "    print('Model downloaded to', test_model_file)\n",
        "    unzip_file(test_model_file, test_model_path)\n",
        "    print('\\n Model is downloaded to ', test_model_path)\n",
        "\n",
        "    # Load the trained model\n",
        "    pickle_file = os.path.join(test_model_path, 'model.sav')\n",
        "    model = load_model(pickle_file)\n",
        "\n",
        "    # Predict using the model\n",
        "    y_discpred = model.predict(X_disctest)\n",
        "    test_disdata['out_label_model_Dis'] = y_discpred\n",
        "\n",
        "    # Save the model output in the same output file\n",
        "    test_disdata.to_csv(test_file, index=False)\n",
        "    print('\\n Output is saved in ', test_file)\n",
        "\n",
        "    return test_file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def val_dis(val_file, MODEL_PATH, model_gdrive_link):\n",
        "    \"\"\"\n",
        "    Test the Discriminative Model on the valid data.\n",
        "\n",
        "    Args:\n",
        "        val_file: Path to the val file containing comments.\n",
        "        MODEL_PATH: Path to the directory containing the trained model.\n",
        "        model_gdrive_link: Google Drive link for the trained model.\n",
        "\n",
        "    Returns:\n",
        "        val_file: Path to the output file containing the test results.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import necessary libraries\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    import pickle\n",
        "\n",
        "    # Clean the test data\n",
        "    val_disdata = data_commentcleaning(val_file)\n",
        "\n",
        "    # TF-IDF VECTORIZER\n",
        "    X_disctval = gendis_vectorizer.transform(val_disdata.comment).toarray()\n",
        "\n",
        "    # Paths for temporary directory and file\n",
        "    test_model_file = MODEL_PATH + '/Model_Dis.zip'\n",
        "    test_model_path = MODEL_PATH + '/Model_Dis/'\n",
        "\n",
        "    # Download and unzip the model file\n",
        "    download_zip_file_from_link(model_gdrive_link, test_model_file)\n",
        "    print('Model downloaded to', test_model_file)\n",
        "    unzip_file(test_model_file, test_model_path)\n",
        "    print('\\n Model is downloaded to ', test_model_path)\n",
        "\n",
        "    # Load the trained model\n",
        "    pickle_file = os.path.join(test_model_path, 'model.sav')\n",
        "    model = load_model(pickle_file)\n",
        "\n",
        "    # Predict using the model\n",
        "    y_discpredva = model.predict(X_disctval)\n",
        "    val_disdata['out_label_model_Dis'] = y_discpredva\n",
        "\n",
        "    # Save the model output in the same output file\n",
        "    val_disdata.to_csv(val_file, index=False)\n",
        "    print('\\n Output is saved in ', val_file)\n",
        "\n",
        "    return val_file\n",
        "\n"
      ],
      "metadata": {
        "id": "oycTQK7eC6dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uizke2YWm8d3"
      },
      "source": [
        "## Discriminative Method  End\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q78eeDWnBVR"
      },
      "source": [
        "# Other Method/model Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVK7bknIm1vu"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "# Define argparse-like function\n",
        "def parse_arguments(option):\n",
        "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
        "    parser.add_argument('--option', '-o',  type=str, default=option, help='Description of your option.')\n",
        "    args = parser.parse_args(args=[])\n",
        "    return args\n",
        "\n",
        "# Function to perform some action based on selected option\n",
        "def perform_action(option):\n",
        "    print(\"Performing action with option:\", option)\n",
        "\n",
        "    if option == '0':\n",
        "      print('\\n Okay Exiting!!! ')\n",
        "\n",
        "    elif option == '1':\n",
        "      print('\\n Training Generative Model')\n",
        "      model_gdrive_link = train_Gen(train_file,val_file,MODEL_Gen_DIRECTORY)\n",
        "      print('Make sure to pass model URL in Testing',model_gdrive_link)\n",
        "\n",
        "    elif option == '2':\n",
        "      print('\\n\\n Pass the URL Not Variable !!!')\n",
        "      print('\\n Testing Generative Model')\n",
        "      model_gen_url = 'https://drive.google.com/file/d/1--TDKFx7qpOuXgauAVi-k9BVM_0RmPly/view?usp=sharing'\n",
        "      test_Gen(test_file,MODEL_PATH ,model_gen_url)\n",
        "\n",
        "    elif option == '3':\n",
        "      print('\\n Training Disciminative Model')\n",
        "      model_gdrive_link = train_dis(train_file,val_file,MODEL_Dis_DIRECTORY)\n",
        "      print('Make sure to pass model URL in Testing',model_gdrive_link)\n",
        "      print('\\n\\n Pass the URL Not Variable !!!')\n",
        "\n",
        "    elif option == '4':\n",
        "      print('\\n\\n Pass the URL Not Variable !!!')\n",
        "      print('\\n Testing Disciminative Model')\n",
        "      model_dis_url = 'https://drive.google.com/file/d/1-WHHgGoKmRt9pG-s2XGx9vV6LXFfAPYo/view?usp=sharing'\n",
        "      test_dis(test_file, MODEL_PATH, model_dis_url)\n",
        "\n",
        "\n",
        "    elif option == '5':\n",
        "      print('\\n\\n Pass the URL Not Variable !!!')\n",
        "      print('\\n validation Disciminative Model')\n",
        "      model_dis_url = 'https://drive.google.com/file/d/1-WHHgGoKmRt9pG-s2XGx9vV6LXFfAPYo/view?usp=sharing'\n",
        "      val_gen(val_file, MODEL_PATH, model_dis_url)\n",
        "\n",
        "\n",
        "    elif option == '6':\n",
        "      print('\\n\\n Pass the URL Not Variable !!!')\n",
        "      print('\\n validation Disciminative Model')\n",
        "      model_dis_url = 'https://drive.google.com/file/d/1-WHHgGoKmRt9pG-s2XGx9vV6LXFfAPYo/view?usp=sharing'\n",
        "      val_dis(val_file, MODEL_PATH, model_dis_url)\n",
        "\n",
        "\n",
        "    else:\n",
        "      print('Wrong Option Selected. \\n\\nPlease select Correct option')\n",
        "      main()\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Get option from user input\n",
        "    user_option = input(\"0. To Exit Code\\n\"\n",
        "                     \"1. Train Model Generative\\n\"\n",
        "                    \"2. Test Model Generative\\n\"\n",
        "                    \"3. Train Model Discriminative\\n\"\n",
        "                    \"4. Test Model Discriminative\\n\"\n",
        "                    \"5. Valid Model Generative\\n\"\n",
        "                    \"6. Valid Model Discriminative\\n\"\n",
        "                    \"Enter your option: \")\n",
        "\n",
        "    args = parse_arguments(user_option)\n",
        "    option = args.option\n",
        "    perform_action(option)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYpZ_p_AnLSU"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NxmLDResfIJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}